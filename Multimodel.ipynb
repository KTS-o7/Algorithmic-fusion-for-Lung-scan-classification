{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset,random_split,DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 16\n",
    "EPOCHS = 10\n",
    "RESNETSIZE = 2048\n",
    "GOOGLENETSIZE = 1024\n",
    "DENSENETSIZE = 1024\n",
    "VGG19SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Device being used is {device}')\n",
    "print(f'Batch size is {BATCHSIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.classes = os.listdir(img_dir)\n",
    "        print(self.classes)\n",
    "        self.images = []\n",
    "        self.class_counts = {class_: 0 for class_ in self.classes}  # Initialize class counts\n",
    "\n",
    "        for class_ in self.classes:\n",
    "            for dirpath, dirnames, filenames in os.walk(os.path.join(img_dir, class_)):\n",
    "                for filename in filenames:\n",
    "                    self.images.append((os.path.join(dirpath, filename), class_))\n",
    "                    self.class_counts[class_] += 1  # Increment class count\n",
    "\n",
    "        print(\"Class counts:\", self.class_counts)  # Print class counts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        img = read_image(img_path).float()  # Convert the images to float\n",
    "        img = img.repeat(3, 1, 1)  # Convert the images to 3 channels\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            print(label)\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(self.classes.index(label))  # Convert class name to class index\n",
    "        return img, label\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitemImg__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        img = read_image(img_path).float()  # Convert the images to float\n",
    "        img = ToPILImage()(img)  # Convert the tensor to a PIL Image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return img, label\n",
    "\n",
    "    \n",
    "    def show_first_images(self, num_images=5):\n",
    "        fig = plt.figure(figsize=(10, num_images * len(self.classes)))\n",
    "        for i, class_ in enumerate(self.classes):\n",
    "            class_images = [img for img, label in self.images if label == class_]\n",
    "            for j in range(num_images):\n",
    "                img = read_image(class_images[j]).float()\n",
    "                img = ToPILImage()(img)\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                ax = fig.add_subplot(len(self.classes), num_images, i * num_images + j + 1)\n",
    "                ax.imshow(img, cmap='gray')\n",
    "                ax.set_title(class_)\n",
    "                ax.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the images to 224 x 224\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize the images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset(img_dir='./DataSetkaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show_first_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset(img_dir='./DataSetkaggle', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "print(\"Training Size : \",train_size,\" Validation Size : \",val_size)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCHSIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(name):\n",
    "    if name.lower() == 'resnet50':\n",
    "        return models.resnet50(weights=True)\n",
    "    elif name.lower() == 'vgg19':\n",
    "        return models.vgg19_bn(weights=True)\n",
    "    elif name.lower() == 'densenet121':\n",
    "        return models.densenet121(weights=True)\n",
    "    elif name.lower() == 'googlenet':\n",
    "        return models.googlenet(weights=True)\n",
    "    elif name.lower() == 'mobilenet':\n",
    "        return models.mobilenet_v3_large(weights=True)\n",
    "    else:\n",
    "        raise ValueError(f'Model {name} not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the feature extraction layers\n",
    "class Concatenate(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(Concatenate, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x)\n",
    "        #print(f\"Shape of ResNet50 output: {x1.shape}\")\n",
    "        x2 = self.model2(x)\n",
    "        #print(f\"Shape of VGG19 output: {x2.shape}\")\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        return torch.cat((x1, x2), dim=1)\n",
    "    \n",
    "# Combine into a new model\n",
    "class fusedModel(nn.Module):\n",
    "    def __init__(self, concatenated, output):\n",
    "        super(fusedModel, self).__init__()\n",
    "        self.concatenated = concatenated\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.concatenated(x)\n",
    "        #print(f\"Shape of concatenated output: {x.shape}\")\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_models('mobilenet')\n",
    "model2 = get_models('googlenet')\n",
    "model1Name = model1.__class__.__name__\n",
    "model2Name = model2.__class__.__name__\n",
    "print(model1Name,model2Name)\n",
    "print(model1.parameters,\"\\n\\n\\n\",model2.parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove output layers :\n",
    "#model1 = nn.Sequential(*list(model1.children())[:-1])\n",
    "#model2 = nn.Sequential(*list(model2.children())[:-1])\n",
    "print(model1.parameters,\"\\n\\n\\n\",model2.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeMapping = {'resnet50': RESNETSIZE, 'googlenet': GOOGLENETSIZE, 'densenet121': DENSENETSIZE, 'vgg19': VGG19SIZE}\n",
    "print(sizeMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = Concatenate(model1,model2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new output layer\n",
    "output = nn.Sequential(\n",
    "    nn.Linear(2000, 100),  # Adjusted the input size here\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, num_epochs=25, patience=5,model_name_Input = None):\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    model_name = model_name_Input  # Get the name of the model\n",
    "    os.makedirs(model_name, exist_ok=True)  # Create a directory named after the model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        running_preds = []\n",
    "        running_labels = []\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "\n",
    "        # Iterate over data\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            #outputs = model(inputs)\n",
    "            try:\n",
    "                outputs = model(inputs)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(f\"Input shape to the linear layer: {inputs.shape}\")\n",
    "                raise e\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print iteration results\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            running_preds.extend(preds.cpu().numpy())\n",
    "            running_labels.extend(labels.data.cpu().numpy())\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append((torch.sum(preds == labels.data).double() / inputs.size(0)).item())\n",
    "            if i % 10 == 0:  # Print every 10 batches\n",
    "                print(f'Batch {i} Loss: {loss.item():.4f} Acc: {torch.sum(preds == labels.data).double() / inputs.size(0):.4f}')\n",
    "\n",
    "        # Plot loss and accuracy vs batch\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(losses)\n",
    "        plt.title('Loss vs Batch')\n",
    "        plt.subplot(122)\n",
    "        plt.plot(accuracies)\n",
    "        plt.title('Accuracy vs Batch')\n",
    "        plt.savefig(f'{model_name}/epoch_{epoch+1}_loss_accuracy.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Print epoch results\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "        epoch_precision = precision_score(running_labels, running_preds, average='macro')\n",
    "        epoch_recall = recall_score(running_labels, running_preds, average='macro')\n",
    "        epoch_f1 = f1_score(running_labels, running_preds, average='macro')\n",
    "        print(f'Epoch Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        print(f'Epoch Precision: {epoch_precision:.4f} Recall: {epoch_recall:.4f} F1-score: {epoch_f1:.4f}\\n')\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(running_labels, running_preds)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig(f'{model_name}/epoch_{epoch+1}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs without improvement.')\n",
    "                return model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fused_model(concatenated,output):\n",
    "    fusedmodel = fusedModel(concatenated, output)\n",
    "    return fusedmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fusedModel = fusedModel(concatenated, output)\n",
    "fusedModel = create_fused_model(concatenated=concatenated,output=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusedModel = fusedModel.to(device)\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer_fusedModel = optim.SGD(fusedModel.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusedModel = train_model(fusedModel, train_dataloader, optimizer_fusedModel, criterion, num_epochs=EPOCHS,model_name_Input=model1Name+\"_\"+model2Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(fusedModel.state_dict(),f'/home/krishnatejaswis/Files/VSCode/LungScan/fusedModels/{model1Name+\"_\"+model2Name+\"_Test\"}.pth')\n",
    "\n",
    "import dill\n",
    "\n",
    "with open(f'/home/krishnatejaswis/Files/VSCode/LungScan/fusedModels/{model1Name+\"_\"+model2Name}.pth', 'wb') as f:\n",
    "    dill.dump(fusedModel, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
